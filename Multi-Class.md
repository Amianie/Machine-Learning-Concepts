# Multi-Class Neural Networks.
One vs. all provides a way to leverage binary classification. Given a classification problem with N possible solutions, a one-vs.-all solution consists of N separate binary classifiersâ€”one binary classifier for each possible outcome. During training, the model runs through a sequence of binary classifiers, training each to answer a separate classification question. For example, given a picture of a dog, five different recognizers might be trained, four seeing the image as a negative example (not an apple, not a bear, etc.) and one seeing the image as a positive example (a dog). 

This approach is fairly reasonable when the total number of classes is small, but becomes increasingly inefficient as the number of classes rises. For example, if you wanted to classify a set of images into 1,000 classes, you'd need to train 1,000 separate binary classifiers.

We can create a significantly more efficient one-vs.-all model with a deep neural network in which each output node represents a different class. 

## Softmax.
Softmax extends this idea into a multi-class world. That is, Softmax assigns decimal probabilities to each class in a multi-class problem. Those decimal probabilities must add up to 1.0. This additional constraint helps training converge more quickly than it otherwise would.

Softmax is implemented through a neural network layer just before the output layer. The Softmax layer must have the same number of nodes as the output layer. Each Softmax node assigns a probability to one of the classes. Those probabilities are then fed to the output layer, which combines them into multinomial distributions over all possible classes.

Softmax Options:
>- Full softmax: The output layer has one node per class in the model, and the values generated by the softmax function sum to 1.0. This approach is good for multi-class classification problems in which a single example can belong to only one class. For example, a handwritten digit can only be a 0, 1, 2, 3, 4, 5, 6, 7, 8, or 9.In other words,softmax calculates the probability for every possible class.
>- Candidate sampling: The output layer has one node per class per candidate set. The values generated by the softmax function for a given example do not necessarily sum to 1.0. This approach is good for multi-class classification problems in which a single example might belong to multiple classes. For example, a movie might be both a romantic comedy and a comedy. In other words, softmax calculates the probability for every class in the candidate set.

Full Softmax is fairly cheap when the number of classes is small but becomes prohibitively expensive when the number of classes climbs. Candidate sampling can improve efficiency in problems having a large number of classes.

## One Label VS Many Label
Softmax assumes that each example is a member of exactly one class. Some examples, however, can simultaneously be a member of multiple classes. For such examples:
>- You may not use softmax.
>- You must rely on multiple Logistic regression.
